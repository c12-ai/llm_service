# ğŸ¬ Docker éƒ¨ç½²æ¼”ç¤º - æœ¬åœ°æµ‹è¯•æ­¥éª¤

## ğŸ“ å®Œæ•´æµç¨‹ï¼ˆ3åˆ†é’Ÿæå®šï¼‰

### æ­¥éª¤ 1: å‡†å¤‡ç¯å¢ƒ (30ç§’)

```bash
# æ£€æŸ¥ Docker æ˜¯å¦å®‰è£…
docker --version
docker ps

# å¦‚æœæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… Docker Desktop:
# macOS: https://www.docker.com/products/docker-desktop
# Linux: curl -fsSL https://get.docker.com | sh
```

### æ­¥éª¤ 2: ä¸‹è½½æ¨¡å‹ (1-2åˆ†é’Ÿï¼Œä»…é¦–æ¬¡éœ€è¦)

```bash
# å®‰è£… modelscope
pip install modelscope

# ä¸‹è½½ Qwen3-4B æ¨¡å‹ï¼ˆçº¦ 8GBï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰
python -c "from modelscope import snapshot_download; snapshot_download('Qwen/Qwen3-4B')"

# ç¡®è®¤æ¨¡å‹å·²ä¸‹è½½
ls -lh ~/.cache/modelscope/hub/models/Qwen/Qwen3-4B
```

### æ­¥éª¤ 3: ä¸€é”®éƒ¨ç½² (1-2åˆ†é’Ÿ)

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /Users/tengpi/vs_code/llm_service

# è¿è¡Œä¸€é”®éƒ¨ç½²è„šæœ¬
./deploy-cpu.sh
```

ä½ ä¼šçœ‹åˆ°ï¼š
```
==========================================
ğŸš€ LLM Service Docker éƒ¨ç½² (CPUç‰ˆæœ¬)
==========================================

ğŸ“¦ æ­¥éª¤ 1/4: æ„å»º Docker é•œåƒ...
ğŸš€ æ­¥éª¤ 2/4: å¯åŠ¨æœåŠ¡...
â³æ­¥éª¤ 3/4: ç­‰å¾…æœåŠ¡å¯åŠ¨ï¼ˆæ¨¡å‹åŠ è½½ä¸­ï¼‰...
âœ… æœåŠ¡å¯åŠ¨æˆåŠŸï¼
ğŸ§ª æ­¥éª¤ 4/4: æµ‹è¯• API...
âœ… API æµ‹è¯•æˆåŠŸï¼

==========================================
ğŸ‰ éƒ¨ç½²å®Œæˆï¼
==========================================
```

### æ­¥éª¤ 4: è¿è¡Œæµ‹è¯• (30ç§’)

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
./test-docker.sh
```

ä½ ä¼šçœ‹åˆ°ï¼š
```
==========================================
ğŸ§ª LLM Service Docker æµ‹è¯•
==========================================

æµ‹è¯• 1/5: å¥åº·æ£€æŸ¥...
  âœ… å¥åº·æ£€æŸ¥é€šè¿‡

æµ‹è¯• 2/5: éæµå¼å¯¹è¯...
  âœ… éæµå¼å¯¹è¯æˆåŠŸ
  æ¨¡å‹å›ç­”: 2

æµ‹è¯• 3/5: å¤šè½®å¯¹è¯ï¼ˆå¸¦å†å²ï¼‰...
  âœ… å¤šè½®å¯¹è¯æˆåŠŸ
  æ¨¡å‹å›ç­”: ä½ å«å°æ˜

æµ‹è¯• 4/5: æµå¼è¾“å‡º...
  âœ… æµå¼è¾“å‡ºæˆåŠŸ

æµ‹è¯• 5/5: OpenAI SDK å…¼å®¹æ€§...
  âœ… OpenAI SDK æµ‹è¯•æˆåŠŸ

==========================================
ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼
==========================================
```

### æ­¥éª¤ 5: å®é™…ä½¿ç”¨ (éšæ—¶)

#### 5.1 ä½¿ç”¨ curl æµ‹è¯•

```bash
# ç®€å•å¯¹è¯
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„åŠ©æ‰‹"},
      {"role": "user", "content": "1+1ç­‰äºå‡ ï¼Ÿ"}
    ],
    "max_tokens": 50
  }'
```

é¢„æœŸå“åº”ï¼š
```json
{
  "id": "chatcmpl-abc12345",
  "object": "chat.completion",
  "created": 1706524800,
  "model": "Qwen3-4B",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "2"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 25,
    "completion_tokens": 3,
    "total_tokens": 28
  }
}
```

#### 5.2 ä½¿ç”¨ Python OpenAI SDK

åˆ›å»ºæ–‡ä»¶ `test_my_service.py`:

```python
import openai

# é…ç½®æŒ‡å‘æœ¬åœ°æœåŠ¡
client = openai.OpenAI(
    api_key="dummy",
    base_url="http://localhost:8000/v1"
)

# æµ‹è¯•1: ç®€å•å¯¹è¯
print("æµ‹è¯•1: ç®€å•å¯¹è¯")
response = client.chat.completions.create(
    model="qwen",
    messages=[
        {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»Python"}
    ],
    max_tokens=100
)
print(f"å›ç­”: {response.choices[0].message.content}\n")

# æµ‹è¯•2: æµå¼è¾“å‡º
print("æµ‹è¯•2: æµå¼è¾“å‡º")
print("æ¨¡å‹å›ç­”: ", end="", flush=True)
stream = client.chat.completions.create(
    model="qwen",
    messages=[{"role": "user", "content": "æ•°åˆ°5"}],
    max_tokens=50,
    stream=True
)
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print("\n")

# æµ‹è¯•3: å¤šè½®å¯¹è¯
print("æµ‹è¯•3: å¤šè½®å¯¹è¯ï¼ˆå¸¦å†å²ï¼‰")
response = client.chat.completions.create(
    model="qwen",
    messages=[
        {"role": "user", "content": "æˆ‘å«å°æ˜"},
        {"role": "assistant", "content": "ä½ å¥½ï¼Œå°æ˜ï¼"},
        {"role": "user", "content": "æˆ‘å«ä»€ä¹ˆï¼Ÿ"}
    ],
    max_tokens=30
)
print(f"å›ç­”: {response.choices[0].message.content}")
print(f"Token ä½¿ç”¨: {response.usage.total_tokens}")
```

è¿è¡Œï¼š
```bash
python test_my_service.py
```

#### 5.3 è®¿é—® API æ–‡æ¡£

æµè§ˆå™¨æ‰“å¼€ï¼š
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

### æ­¥éª¤ 6: æŸ¥çœ‹æ—¥å¿—å’ŒçŠ¶æ€

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
docker logs -f qwen-llm-service-cpu

# æŸ¥çœ‹èµ„æºä½¿ç”¨
docker stats qwen-llm-service-cpu

# æŸ¥çœ‹å®¹å™¨çŠ¶æ€
docker ps | grep qwen
```

### æ­¥éª¤ 7: åœæ­¢æœåŠ¡

```bash
# åœæ­¢å¹¶åˆ é™¤å®¹å™¨
docker-compose -f docker-compose.cpu.yml down

# æˆ–è€…åªåœæ­¢ï¼ˆä¿ç•™å®¹å™¨ï¼‰
docker stop qwen-llm-service-cpu
```

## ğŸ¯ æ€»ç»“ï¼šä½ å·²ç»å®Œæˆäº†

âœ… Docker é•œåƒæ„å»º  
âœ… å®¹å™¨åŒ–éƒ¨ç½²  
âœ… æœåŠ¡å¯åŠ¨å’Œå¥åº·æ£€æŸ¥  
âœ… API æµ‹è¯•ï¼ˆéæµå¼ + æµå¼ï¼‰  
âœ… OpenAI SDK å…¼å®¹æ€§éªŒè¯  

## ğŸ“‹ å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‘½ä»¤
./docker-quick-ref.sh

# éƒ¨ç½²
./deploy-cpu.sh

# æµ‹è¯•
./test-docker.sh

# æ—¥å¿—
docker logs -f qwen-llm-service-cpu

# åœæ­¢
docker-compose -f docker-compose.cpu.yml down

# é‡å¯
docker-compose -f docker-compose.cpu.yml restart
```

## ğŸš€ ä¸‹ä¸€æ­¥

ç°åœ¨ä½ çš„æœåŠ¡å·²ç»åœ¨æœ¬åœ°è¿è¡Œäº†ï¼Œå¯ä»¥ï¼š

1. **é›†æˆåˆ°å…¶ä»–é¡¹ç›®**
   - ä¿®æ”¹å®¢æˆ·ç«¯çš„ `base_url` ä¸º `http://localhost:8000/v1`
   - å®Œå…¨å…¼å®¹ OpenAI APIï¼Œæ— éœ€ä¿®æ”¹å…¶ä»–ä»£ç 

2. **åˆ‡æ¢åˆ° GPU ç‰ˆæœ¬**ï¼ˆå¦‚æœæœ‰ GPUï¼‰
   ```bash
   docker-compose up -d  # ä½¿ç”¨ GPU ç‰ˆæœ¬é…ç½®
   ```

3. **éƒ¨ç½²åˆ°æœåŠ¡å™¨**
   - ä¿®æ”¹ `docker-compose.cpu.yml` ä¸­çš„ç«¯å£æ˜ å°„
   - ä½¿ç”¨ Nginx åå‘ä»£ç†
   - æ·»åŠ  HTTPS è¯ä¹¦

4. **æ‰©å±•åŠŸèƒ½**
   - æ·»åŠ  API Key è®¤è¯
   - é›†æˆ Prometheus ç›‘æ§
   - å®ç°å¤šæ¨¡å‹åˆ‡æ¢
   - æ·»åŠ è¯·æ±‚é˜Ÿåˆ—å’Œé™æµ

## ğŸ“– æ›´å¤šèµ„æº

- [Docker è¯¦ç»†éƒ¨ç½²æŒ‡å—](DOCKER_DEPLOYMENT.md)
- [OpenAI API å…¼å®¹æ€§è¯´æ˜](OPENAI_COMPATIBILITY.md)
- [é¡¹ç›® README](README.md)

---

**æ­å–œï¼** ä½ å·²ç»æˆåŠŸéƒ¨ç½²å¹¶æµ‹è¯•äº†åŸºäº Docker çš„ LLM æœåŠ¡ï¼ğŸ‰
