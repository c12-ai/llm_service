# Qwen LLM Service

åŸºäº Qwen å¤§æ¨¡å‹çš„æ¨ç†æœåŠ¡ï¼Œ**å®Œå…¨å…¼å®¹ OpenAI API**ï¼Œæ”¯æŒ CPU å’Œ GPU éƒ¨ç½²ã€‚

## âœ¨ ç‰¹æ€§

- âœ… **OpenAI API å®Œå…¨å…¼å®¹** - æ— ç¼æ›¿æ¢ OpenAI æœåŠ¡
- âœ… **æµå¼è¾“å‡ºæ”¯æŒ** - å®æ—¶è¿”å›ç”Ÿæˆå†…å®¹
- âœ… **å¤šç§éƒ¨ç½²æ–¹å¼** - æœ¬åœ°/Docker/GPU/CPU
- âœ… **Token ç»Ÿè®¡** - å®Œæ•´çš„ä½¿ç”¨é‡ç»Ÿè®¡
- âœ… **ç”Ÿäº§å°±ç»ª** - å¥åº·æ£€æŸ¥ã€é”™è¯¯å¤„ç†ã€æ—¥å¿—è®°å½•

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼1: Docker éƒ¨ç½²ï¼ˆæ¨èï¼‰â­

```bash
# 1. ä¸‹è½½æ¨¡å‹
python -c "from modelscope import snapshot_download; snapshot_download('Qwen/Qwen3-4B')"

# 2. ä¸€é”®éƒ¨ç½²
./deploy-cpu.sh

# 3. æµ‹è¯•æœåŠ¡
./test-docker.sh
```

è¯¦è§ï¼š[Docker å¿«é€Ÿå¼€å§‹](DOCKER_QUICKSTART.md)

### æ–¹å¼2: æœ¬åœ°è¿è¡Œï¼ˆå¼€å‘æµ‹è¯•ï¼‰

```bash
# 1. å®‰è£…ä¾èµ–
pip install -r requirements-cpu.txt

# 2. ä¸‹è½½æ¨¡å‹
python -c "from modelscope import snapshot_download; snapshot_download('Qwen/Qwen3-4B')"

# 3. å¯åŠ¨æœåŠ¡
python llm_service_cpu.py

# 4. æµ‹è¯•
python test_openai_compatibility.py
```

## ğŸ“– æ–‡æ¡£

| æ–‡æ¡£ | è¯´æ˜ |
|------|------|
| [DOCKER_QUICKSTART.md](DOCKER_QUICKSTART.md) | Docker å¿«é€Ÿå¼€å§‹ï¼ˆæ¨èï¼‰ |
| [DOCKER_DEPLOYMENT.md](DOCKER_DEPLOYMENT.md) | Docker è¯¦ç»†éƒ¨ç½²æŒ‡å— |
| [MODEL_SWITCH.md](MODEL_SWITCH.md) | **æ¨¡å‹å¿«é€Ÿåˆ‡æ¢æŒ‡å—** â­ |
| [OPENAI_COMPATIBILITY.md](OPENAI_COMPATIBILITY.md) | OpenAI API å…¼å®¹æ€§è¯´æ˜ |

## ğŸ”„ å¿«é€Ÿåˆ‡æ¢æ¨¡å‹

åªéœ€ 3 æ­¥ï¼Œæ— éœ€ä¿®æ”¹ä»£ç ï¼š

```bash
# 1. å¤åˆ¶é…ç½®æ¨¡æ¿ï¼ˆé¦–æ¬¡ï¼‰
cp .env.example .env

# 2. ç¼–è¾‘ .envï¼Œä¿®æ”¹æ¨¡å‹è·¯å¾„
# MODEL_PATH=/Users/tengpi/.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct
# MODEL_NAME=Qwen2.5-7B-Instruct

# 3. é‡å¯æœåŠ¡
docker-compose -f docker-compose.cpu.yml restart
```

è¯¦è§ï¼š[MODEL_SWITCH.md](MODEL_SWITCH.md)
| [DOCKER_DEPLOYMENT.md](DOCKER_DEPLOYMENT.md) | Docker è¯¦ç»†éƒ¨ç½²æŒ‡å— |
| [OPENAI_COMPATIBILITY.md](OPENAI_COMPATIBILITY.md) | OpenAI API å…¼å®¹æ€§è¯´æ˜ |

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### Python (OpenAI SDK)

```python
import openai

client = openai.OpenAI(
    api_key="dummy",  # æœ¬åœ°æœåŠ¡ä¸éœ€è¦çœŸå® key
    base_url="http://localhost:8000/v1"
)

# ç®€å•å¯¹è¯
response = client.chat.completions.create(
    model="qwen",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"},
        {"role": "user", "content": "ä½ å¥½"}
    ],
    max_tokens=100
)
print(response.choices[0].message.content)

# æµå¼è¾“å‡º
stream = client.chat.completions.create(
    model="qwen",
    messages=[{"role": "user", "content": "æ•°åˆ°10"}],
    stream=True
)
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

### curl

```bash
# éæµå¼
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "max_tokens": 50
  }'

# æµå¼
curl -N -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen",
    "messages": [{"role": "user", "content": "æ•°åˆ°5"}],
    "stream": true
  }'
```

## ğŸ“Š API ç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | è¯´æ˜ |
|------|------|------|
| `/health` | GET | å¥åº·æ£€æŸ¥ |
| `/v1/chat/completions` | POST | èŠå¤©æ¥å£ï¼ˆOpenAI å…¼å®¹ï¼‰ |
| `/generate` | POST | æ–‡æœ¬ç”Ÿæˆæ¥å£ï¼ˆç®€å•ç‰ˆï¼‰ |
| `/docs` | GET | API æ–‡æ¡£ï¼ˆSwagger UIï¼‰ |

## ğŸ³ Docker éƒ¨ç½²

### CPU ç‰ˆæœ¬ï¼ˆæœ¬åœ°æµ‹è¯•ï¼‰

```bash
# ä¸€é”®éƒ¨ç½²
./deploy-cpu.sh

# æˆ–æ‰‹åŠ¨éƒ¨ç½²
docker-compose -f docker-compose.cpu.yml up -d

# æŸ¥çœ‹æ—¥å¿—
docker logs -f qwen-llm-service-cpu

# åœæ­¢æœåŠ¡
docker-compose -f docker-compose.cpu.yml down
```

### GPU ç‰ˆæœ¬ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

```bash
docker-compose up -d
```

## âš™ï¸ é…ç½®

ç¼–è¾‘ `config.py`:

```python
# æ¨¡å‹é…ç½®
MODEL_PATH = "/path/to/your/model"
MODEL_NAME = "your-model-name"

# æœåŠ¡é…ç½®
SERVICE_HOST = "0.0.0.0"
SERVICE_PORT = 8000
```

æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼š

```bash
export MODEL_PATH="/path/to/model"
export MODEL_NAME="qwen"
export SERVICE_PORT=8000
```

## ğŸ§ª æµ‹è¯•

```bash
# Docker æµ‹è¯•
./test-docker.sh

# æœ¬åœ°æµ‹è¯•
python test_openai_compatibility.py

# ç®€å•æµ‹è¯•
curl http://localhost:8000/health
```

## ğŸ“ˆ æ€§èƒ½

### CPU æ¨¡å¼ï¼ˆæœ¬åœ°æµ‹è¯•ï¼‰
- å¯åŠ¨æ—¶é—´ï¼š1-2 åˆ†é’Ÿ
- æ¨ç†é€Ÿåº¦ï¼š10-30 ç§’/è¯·æ±‚
- é€‚ç”¨åœºæ™¯ï¼šå¼€å‘æµ‹è¯•

### GPU æ¨¡å¼ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
- å¯åŠ¨æ—¶é—´ï¼š30-60 ç§’
- æ¨ç†é€Ÿåº¦ï¼š1-3 ç§’/è¯·æ±‚
- é€‚ç”¨åœºæ™¯ï¼šç”Ÿäº§éƒ¨ç½²

## ğŸ”„ ä» OpenAI è¿ç§»

**é›¶ä»£ç æ”¹åŠ¨ï¼** åªéœ€ä¿®æ”¹ä¸¤è¡Œé…ç½®ï¼š

```python
# åŸæ¥
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# æ”¹æˆ
client = openai.OpenAI(
    api_key="dummy",
    base_url="http://your-server:8000/v1"
)
```

å…¶ä»–ä»£ç å®Œå…¨ä¸ç”¨æ”¹ï¼è¯¦è§ [OPENAI_COMPATIBILITY.md](OPENAI_COMPATIBILITY.md)

## ğŸ› ï¸ å¼€å‘

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo>
cd llm_service

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements-cpu.txt

# è¿è¡ŒæœåŠ¡
python llm_service_cpu.py
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
llm_service/
â”œâ”€â”€ llm_service_cpu.py          # CPU ç‰ˆæœ¬æœåŠ¡ï¼ˆTransformersï¼‰
â”œâ”€â”€ llm_service.py              # GPU ç‰ˆæœ¬æœåŠ¡ï¼ˆvLLMï¼‰
â”œâ”€â”€ config.py                   # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements-cpu.txt        # CPU ç‰ˆæœ¬ä¾èµ–
â”œâ”€â”€ requirements.txt            # GPU ç‰ˆæœ¬ä¾èµ–
â”œâ”€â”€ Dockerfile.cpu              # CPU Docker é•œåƒ
â”œâ”€â”€ Dockerfile                  # GPU Docker é•œåƒ
â”œâ”€â”€ docker-compose.cpu.yml      # CPU Docker Compose
â”œâ”€â”€ docker-compose.yml          # GPU Docker Compose
â”œâ”€â”€ deploy-cpu.sh â­           # ä¸€é”®éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ test-docker.sh â­          # Docker æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_openai_compatibility.py # OpenAI å…¼å®¹æ€§æµ‹è¯•
â””â”€â”€ *.md                        # æ–‡æ¡£
```

## â“ å¸¸è§é—®é¢˜

### Q: æœåŠ¡å¯åŠ¨å¾ˆæ…¢ï¼Ÿ
A: æ­£å¸¸ç°è±¡ã€‚æ¨¡å‹åŠ è½½éœ€è¦ 1-2 åˆ†é’Ÿï¼ˆCPUï¼‰æˆ– 30-60 ç§’ï¼ˆGPUï¼‰ã€‚

### Q: CPU æ¨¡å¼æ¨ç†å¾ˆæ…¢ï¼Ÿ
A: CPU æ¨ç†æœ¬èº«å°±æ…¢ï¼Œä»…ç”¨äºæµ‹è¯•ã€‚ç”Ÿäº§ç¯å¢ƒè¯·ä½¿ç”¨ GPUã€‚

### Q: å¦‚ä½•æ›´æ¢æ¨¡å‹ï¼Ÿ
A: ä¿®æ”¹ `config.py` ä¸­çš„ `MODEL_PATH` å’Œ `MODEL_NAME`ã€‚

### Q: æ”¯æŒå“ªäº›æ¨¡å‹ï¼Ÿ
A: æ‰€æœ‰ Hugging Face/ModelScope ä¸Šçš„ Causal LM æ¨¡å‹ï¼ˆå¦‚ Qwenã€LLaMA ç­‰ï¼‰ã€‚

### Q: å¦‚ä½•æ·»åŠ  API Key è®¤è¯ï¼Ÿ
A: åœ¨ FastAPI ä¸­æ·»åŠ ä¸­é—´ä»¶ï¼ŒéªŒè¯è¯·æ±‚å¤´ä¸­çš„ API Keyã€‚

## ğŸ“„ è®¸å¯è¯

MIT

## ğŸ™ è‡´è°¢

- [Qwen](https://github.com/QwenLM/Qwen) - åŸºåº§æ¨¡å‹
- [vLLM](https://github.com/vllm-project/vllm) - GPU æ¨ç†å¼•æ“
- [Transformers](https://github.com/huggingface/transformers) - CPU æ¨ç†æ”¯æŒ
- [FastAPI](https://fastapi.tiangolo.com/) - Web æ¡†æ¶
